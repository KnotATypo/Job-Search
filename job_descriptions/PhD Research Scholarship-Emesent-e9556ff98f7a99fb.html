<div class="jobsearch-JobComponent-description css-10ybyod eu4oa1w0">
 <div class="mosaic-zone" id="mosaic-vjBelowViewJobHeader">
 </div>
 <div class="mosaic-zone" data-testid="vjProfileInsights-test" id="mosaic-vjProfileInsights">
 </div>
 <div class="mosaic-zone" id="mosaic-belowVjProfileInsights">
 </div>
 <div class="mosaic-zone" data-testid="vjJobDetails-test" id="mosaic-vjJobDetails">
  <div class="mosaic js-match-insights-provider-job-details" id="js-match-insights-provider-job-details">
   <div class="css-kyg8or eu4oa1w0">
    <div class="js-match-insights-provider-kyg8or eu4oa1w0" id="jobDetailsSection">
     <div class="js-match-insights-provider-xu0md5 eu4oa1w0">
      <div class="js-match-insights-provider-36vfsm eu4oa1w0">
       <div class="js-match-insights-provider-1283b4z eu4oa1w0">
        <h2 class="js-match-insights-provider-14dlqhn e1tiznh50">
         Job details
        </h2>
        <span class="js-match-insights-provider-xvmbeo e1wnkr790">
         <span>
          Here’s how the job details align with your
          <a aria-label="job preferences (opens in a new window)" class="js-match-insights-provider-1cr09u7 e19afand0" href="https://profile.indeed.com/" rel="noopener" target="_blank">
           profile
           <svg aria-hidden="true" class="js-match-insights-provider-r5jz5s eac13zx0" fill="currentColor" focusable="false" role="img" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
            <path d="M14.504 3a.5.5 0 00-.5.5v1a.5.5 0 00.5.5h3.085l-9.594 9.594a.5.5 0 000 .707l.707.708a.5.5 0 00.707 0l9.594-9.595V9.5a.5.5 0 00.5.5h1a.5.5 0 00.5-.5v-6a.5.5 0 00-.5-.5h-6z">
            </path>
            <path d="M5 3.002a2 2 0 00-2 2v13.996a2 2 0 001.996 2.004h14a2 2 0 002-2v-6.5a.5.5 0 00-.5-.5h-1a.5.5 0 00-.5.5v6.5L5 18.998V5.002L11.5 5a.495.495 0 00.496-.498v-1a.5.5 0 00-.5-.5H5z">
            </path>
           </svg>
          </a>
          .
         </span>
        </span>
       </div>
       <div class="js-match-insights-provider-h05mm8 e37uo190">
        <div aria-label="Job type" class="js-match-insights-provider-16m282m e37uo190" role="group" tabindex="0">
         <svg aria-hidden="true" class="js-match-insights-provider-1pdva1a eac13zx0" data-testid="section-icon" fill="currentColor" focusable="false" role="img" viewbox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
          <path clip-rule="evenodd" d="M10 3C7 3 6 6 6 6H2.5a.5.5 0 00-.5.5V9h16V6.5a.5.5 0 00-.5-.5H14s-1-3-4-3zm2.5 3h-5s1-1.5 2.5-1.5S12.5 6 12.5 6z" fill-rule="evenodd">
          </path>
          <path d="M8 11H2v5.5a.5.5 0 00.5.5h15a.5.5 0 00.5-.5V11h-6c0 1-1 2-2 2s-2-1-2-2z">
          </path>
         </svg>
         <div class="js-match-insights-provider-e6s05i eu4oa1w0">
          <h3 class="js-match-insights-provider-11n8e9a e1tiznh50">
           Job type
          </h3>
          <ul class="js-match-insights-provider-1o7r14h eu4oa1w0">
           <li class="js-match-insights-provider-ymm9h5 eu4oa1w0" data-testid="list-item">
            <div class="js-match-insights-provider-1q88txe e1xnxm2i0" data-testid="Full-time-tile">
             <div class="js-match-insights-provider-g6kqeb ecydgvn0">
              <div class="js-match-insights-provider-tvvxwd ecydgvn1">
               Full-time
              </div>
              <div class="js-match-insights-provider-tvvxwd ecydgvn1">
              </div>
             </div>
            </div>
           </li>
          </ul>
         </div>
        </div>
       </div>
       <div class="js-match-insights-provider-bbq8li eu4oa1w0">
       </div>
      </div>
      <div aria-orientation="horizontal" class="js-match-insights-provider-1lugej0 e15p7aqh1" role="separator">
       <span class="js-match-insights-provider-1b6omqv esbq1260">
        <span>
         &amp;nbsp;
        </span>
       </span>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div class="mosaic-zone" id="mosaic-aboveFullJobDescription">
 </div>
 <div class="mosaic-zone" id="mosaic-aboveExtractedJobDescription">
 </div>
 <div class="mosaic-zone" id="mosaic-ssrVJModals">
  <div class="mosaic mosaic-provider-passport-intercept" id="mosaic-provider-passport-intercept">
   <div data-testid="passport-intercept-type-modal">
   </div>
  </div>
 </div>
 <div class="css-1iyxsfc eu4oa1w0" id="jobLocationSectionWrapper">
  <h2 class="css-1yytfzy e1tiznh50" id="jobLocationSectionTitle">
   Location
  </h2>
  <div class="css-1u9px01 e37uo190" id="jobLocationWrapper">
   <svg aria-hidden="true" class="css-r72e78 eac13zx0" fill="currentColor" focusable="false" id="jobLocationIcon" role="img" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
    <path d="M12 2C8.13 2 5 5.13 5 9c0 4.523 5.195 11.093 6.634 12.826a.47.47 0 00.732 0C13.805 20.093 19 13.523 19 9c0-3.87-3.13-7-7-7zm0 9.5a2.5 2.5 0 010-5 2.5 2.5 0 010 5z">
    </path>
   </svg>
   <div class="css-1tlxeot eu4oa1w0" id="jobLocationText">
    <div class="css-45str8 eu4oa1w0" data-testid="jobsearch-JobInfoHeader-companyLocation">
     <span>
      Brisbane QLD
     </span>
    </div>
   </div>
  </div>
  <div aria-orientation="horizontal" class="css-1shvrmd e15p7aqh1" role="separator">
   <span class="css-1b6omqv esbq1260">
    <span>
     &amp;nbsp;
    </span>
   </span>
  </div>
 </div>
 <div id="jobDescriptionTitle">
  <h2 class="css-wpzt8u e1tiznh50" id="jobDescriptionTitleHeading" tabindex="-1">
   Full job description
  </h2>
 </div>
 <div class="jobsearch-jobDescriptionText jobsearch-JobComponent-description css-kqe8pq eu4oa1w0" id="jobDescriptionText">
  <div>
   <div>
    <div>
     <b>
      Accurately mapping large-scale infrastructure assets (power poles, bridges, buildings, whole suburbs and cities) is still exceptionally challenging for robots.
     </b>
    </div>
    <div>
    </div>
    <div>
     <br/>
     The problem becomes even harder when we ask robots to map structures with intricate geometry or when the appearance or the structure of the environment changes over time, for example due to corrosion or construction activity.
    </div>
    <div>
     The problem difficulty is increased even more when sensor data from a range of different sensors (e.g. lidars and cameras, but also more specialised hardware such as gas sensors) need to be integrated; and when the sensor data is gathered by multiple heterogeneous agents (e.g. robots, drones, or human-operated sensor platforms of different kinds).
    </div>
    <div>
     Extracting insights and knowledge from the created maps is another ongoing challenge, especially when the requested insights are of semantic or similar high-level nature, or not even fully known at the time of creating the representation.
    </div>
    <div>
    </div>
    <div>
     <b>
      <br/>
      This PhD project lets you develop new algorithms that enable robots to better map, represent, and understand the world around them.
     </b>
    </div>
    <div>
    </div>
    <div>
     <br/>
     You can solve this problem in close collaboration with researchers from the QUT Centre for Robotics, our Industry Partner Emesent, and researchers from the University of Sydney, the Australian National University, and the Australian Robotic Inspection and Asset Management Hub (ARIAM).
    </div>
    <div>
    </div>
    <div>
     <br/>
     ARIAM is a 5-year, $10 million research project with the University of Sydney, the QUT Centre for Robotics, the Australian National University, and over 10 industry partners. You will have the opportunity to work with researchers from the involved institutions and participate in a range of exciting professional development activities.
    </div>
    <div>
    </div>
    <div>
     <br/>
     You will also be part of the QUT Centre for Robotics, which offers a vibrant research culture with a variety of social and professional activities, ranging from PhD boardgame nights to short courses on professional skills such as presenting, academic writing, managing your time, preparing a CV, or preparing for job interviews.
    </div>
   </div>
   <div>
   </div>
   <div>
    <div>
     <h3 class="jobSectionHeader">
      <b>
       <br/>
       Research activities
      </b>
     </h3>
     <ul>
      <ul>
       <li>
        This project aims to investigate novel algorithms that can efficiently construct and maintain an implicit neural field representation from diverse sensor data such as lidars and cameras, but also more specialised hardware such as gas sensors. The resulting representation is spatio-temporal, i.e. it not only represents the 3D spatial structure of an environment but integrates a temporal dimension, allowing to integrate sensor data taken at different points in time. This project will investigate how sensor data from multiple heterogeneous robotic platforms can be utilised by the implicit representation.
       </li>
       <li>
        Furthermore, the project aims to develop new algorithms to extract high-level semantically meaningful information and insights from the resulting representation of the environment, including identifying relevant changes in the environment over time. The nature of these insights and information will vary across different concrete applications.
       </li>
       <li>
        We will recruit two PhD students with a strong background in machine learning and robotic vision. Each PhD student will be responsible for working towards one of the two aims described above.
       </li>
       <li>
        Within the first 3 months, both students will conduct literature reviews and formulate concrete research questions and a research plan under the guidance of the supervisory team and a representative of Emesent.
       </li>
       <li>
        Following this initial stage, PhD1 will investigate the efficacy of implicit representations such as neural fields to integrate multi-modal sensor data in large-scale environments.
       </li>
       <div>
        <b>
         Considered goals could include:
        </b>
       </div>
       <li>
        extension of existing neural field representations to include multi-modal sensor data beyond the ubiquitous cameras and dense depth data (e.g. sparse depth, gas sensors and others)
       </li>
       <li>
        development of approaches to allow for incremental updates of such a representation
       </li>
       <li>
        development of a new neural field representation that explicitly handles time as an input dimension, thus resulting in a 4D spatio-temporal representation.
       </li>
       <div>
        <b>
         Simultaneously, PhD2 will investigate how high-level semantic knowledge and insights can be extracted from a large-scale implicit representation. Specific goals will be developed in close consultation with Emesent, but could include:
        </b>
       </div>
       <li>
        detecting change in the scene (geometry, semantics, or appearance) over time
       </li>
       <li>
        developing a learning-based approach to distinguish relevant from irrelevant change
       </li>
       <li>
        predicting change over time, and into the future.
       </li>
      </ul>
     </ul>
    </div>
   </div>
   <div>
    <div>
     <h3 class="jobSectionHeader">
      <b>
       Outcomes
      </b>
     </h3>
     <ul>
      <ul>
       <li>
        This project will generate new knowledge and insights into the efficacy of using implicit representations such as neural fields for mapping large-scale environments with multi-modal and multi-agent data. The project would deliver reports, publications, datasets, and a variety of research-grade implementation and source code for the developed methods.
       </li>
       <li>
        These outputs could be translated into new capabilities for the project partner’s business, giving the industry partner a first-to-market position advantage. In addition, the project trains two PhD researchers with expert knowledge in an area of key importance to the industry partner.
       </li>
      </ul>
     </ul>
    </div>
   </div>
   <div>
    <div>
     <h3 class="jobSectionHeader">
      <b>
       Skills &amp; Expereince
      </b>
     </h3>
     <ul>
      <ul>
       <li>
        Applicants should have a strong background in machine learning and robotic vision as well as strong programming skills in Python. Good teamwork, communication (verbal and written), and a strong intrinsic motivation are essential.
       </li>
      </ul>
     </ul>
    </div>
   </div>
   <div>
    <div>
     <b>
      About QUT
     </b>
    </div>
    <div>
     The QUT Centre for Robotics (QCR) conducts at-scale world-leading research in intelligent robotics; translates fundamental research into commercial and societal outcomes; is a leader in education, training and development of talent to meet growing demands for expertise in robotics and autonomous systems; and provides leadership in technological policy development and societal debate. Established in 2020, the centre has been built on the momentum of a decade’s investment in robotic research and translation at QUT which has been funded by QUT, ARC, Queensland Government, CRCs and Industry. QCR comprises over 100 researchers and engineers.
    </div>
    <div>
    </div>
    <div>
     <br/>
     QCR researchers collaborate with industry and universities around the world, including MIT, Harvard and Oxford universities, Boeing, Thales, DST, Airservices Australia, CASA, JARUS, TRAFI, Google Deepmind, Google AI, Amazon Robotics, Caterpillar, Rheinmetall, US Air Force, and NASA’s Jet Propulsion Laboratory.
    </div>
    <div>
    </div>
    <div>
     <br/>
     We are proud of our beautiful and big modern lab space and research environment. We have a fantastic collection of equipment to support your research, including many mobile robot platforms and robotic arms.
    </div>
    <div>
     The centre supports a flexible working environment. We support a diverse and inclusive atmosphere and encourage applications from women, Aboriginal Australians and Torres Strait Islander people.
    </div>
    <div>
    </div>
    <div>
     <b>
      <br/>
      Scholarships: Two fully-funded positions available
     </b>
    </div>
    <div>
     We have two fully-funded ($40,000 per year tax-free) PhD positions available for this project.
    </div>
   </div>
  </div>
 </div>
 <div aria-orientation="horizontal" class="css-1cflm8p e15p7aqh1" role="separator">
  <span class="css-1b6omqv esbq1260">
   <span>
    &amp;nbsp;
   </span>
  </span>
 </div>
</div>
